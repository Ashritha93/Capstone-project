{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Attrition in an Organization || Why Workers Quit?\n\nEmployees are the backbone of the organization. Organization's performance is heavily based on the quality of the employees. Challenges that an organization has to face due employee attrition are:\n\n1. Expensive in terms of both money and time to train new employees.\n1. Loss of experienced employees\n1. Impact in productivity\n1. Impact profit\n\nBefore getting our hands dirty with the data, first step is to frame the business question. Having clarity on below questions is very crucial because the solution that is being developed will make sense only if we have well stated problem.\n\n### Business questions to brainstorm:\n1. What factors are contributing more to employee attrition?\n1. What type of measures should the company take in order to retain their employees?\n1. What business value does the model bring?\n1. Will the model save lots of money?\n1. Which business unit faces the attrition problem?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\n\npd.set_option(\"display.float_format\", \"{:.2f}\".format)\npd.set_option(\"display.max_columns\", 80)\npd.set_option(\"display.max_rows\", 80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"C:/Users/ASHRITHA/Downloads/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Exploratory Data Analysis\n\n- Find patterns in data through data visualization. Reveal hidden secrets of the data through graphs, analysis and charts.\n    - Univariate analysis\n        - Continous variables : Histograms, boxplots. This gives us understanding about the central tendency and spread\n        - Categorical variable : Bar chart showing frequency in each category\n    - Bivariate analysis\n        - Continous & Continous : Scatter plots to know how continous variables interact with each other\n        - Categorical & categorical : Stacked column chart to show how the frequencies are spread between two\n        - categorical variables\n        - Categorical & Continous : Boxplots, Swamplots or even bar charts\n- Detect outliers\n- Feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in df.columns:\n    print(f\"{column}: Number of unique values {df[column].nunique()}\")\n    print(\"==========================================================\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice that '`EmployeeCount`', '`Over18`', '`StandardHours`' have only one unique values and '`EmployeeNumber`' has `1470` unique values.\nThis features aren't useful for us, So we are going to drop those columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], axis=\"columns\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"object_col = []\nfor column in df.columns:\n    if df[column].dtype == object and len(df[column].unique()) <= 30:\n        object_col.append(column)\n        print(f\"{column} : {df[column].unique()}\")\n        print(df[column].value_counts())\n        print(\"====================================\")\nobject_col.remove('Attrition')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(object_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel = LabelEncoder()\ndf[\"Attrition\"] = label.fit_transform(df.Attrition)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numerical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"disc_col = []\nfor column in df.columns:\n    if df[column].dtypes != object and df[column].nunique() < 30:\n        print(f\"{column} : {df[column].unique()}\")\n        disc_col.append(column)\n        print(\"====================================\")\ndisc_col.remove('Attrition')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_col = []\nfor column in df.columns:\n    if df[column].dtypes != object and df[column].nunique() > 30:\n        print(f\"{column} : Minimum: {df[column].min()}, Maximum: {df[column].max()}\")\n        cont_col.append(column)\n        print(\"====================================\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 20))\n\nfor i, column in enumerate(disc_col, 1):\n    plt.subplot(4, 4, i)\n    df[df[\"Attrition\"] == 0][column].hist(bins=35, color='blue', label='Attrition = NO', alpha=0.6)\n    df[df[\"Attrition\"] == 1][column].hist(bins=35, color='red', label='Attrition = YES', alpha=0.6)\n    plt.legend()\n    plt.xlabel(column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that `EnvironmentSatisfaction`, `JobSatisfaction`, `PerformanceRating`, and `RelationshipSatisfaction` features don't have big impact on the detrmination of `Attrition` of employees."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\n\nfor i, column in enumerate(cont_col, 1):\n    plt.subplot(2, 4, i)\n    df[df[\"Attrition\"] == 0][column].hist(bins=35, color='blue', label='Attrition = NO', alpha=0.6)\n    df[df[\"Attrition\"] == 1][column].hist(bins=35, color='red', label='Attrition = YES', alpha=0.6)\n    plt.legend()\n    plt.xlabel(column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusions:**\n\n***\n- The workers with low `JobLevel`, `MonthlyIncome`, `YearAtCompany`, and `TotalWorkingYears` are more likely to quit there jobs.\n- `BusinessTravel` : The workers who travel alot are more likely to quit then other employees.\n\n- `Department` : The worker in `Research & Development` are more likely to stay then the workers on other departement.\n\n- `EducationField` : The workers with `Human Resources` and `Technical Degree` are more likely to quit then employees from other fields of educations.\n\n- `Gender` : The `Male` are more likely to quit.\n\n- `JobRole` : The workers in `Laboratory Technician`, `Sales Representative`, and `Human Resources` are more likely to quit the workers in other positions.\n\n- `MaritalStatus` : The workers who have `Single` marital status are more likely to quit the `Married`, and `Divorced`.\n\n- `OverTime` : The workers who work more hours are likely to quit then others.\n\n*** "},{"metadata":{},"cell_type":"markdown","source":"# 3. Correlation Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30, 30))\nsns.heatmap(df.corr(), annot=True, cmap=\"RdYlGn\", annot_kws={\"size\":15})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = df.corr().nlargest(20, \"Attrition\").Attrition.index\nplt.figure(figsize=(15, 15))\nsns.heatmap(df[col].corr(), annot=True, cmap=\"RdYlGn\", annot_kws={\"size\":10})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('Attrition', axis=1).corrwith(df.Attrition).plot(kind='barh', figsize=(10, 7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis of correlation results (sample analysis):**\n- Monthly income is highly correlated with Job level.\n- Job level is highly correlated with total working hours.\n- Monthly income is highly correlated with total working hours.\n- Age is also positively correlated with the Total working hours.\n- Marital status and stock option level are negatively correlated"},{"metadata":{},"cell_type":"markdown","source":"# 4. Data Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform categorical data into dummies\ndummy_col = [column for column in df.drop('Attrition', axis=1).columns if df[column].nunique() < 20]\ndata = pd.get_dummies(df, columns=dummy_col, drop_first=True, dtype='uint8')\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)\n\n# Remove duplicate Features\ndata = data.T.drop_duplicates()\ndata = data.T\n\n# Remove Duplicate Rows\ndata.drop_duplicates(inplace=True)\n\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop('Attrition', axis=1).corrwith(data.Attrition).sort_values().plot(kind='barh', figsize=(10, 30))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_correlation = data.drop('Attrition', axis=1).corrwith(data.Attrition).sort_values()\nmodel_col = feature_correlation[np.abs(feature_correlation) > 0.02].index\nlen(model_col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Applying machine learning algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX = data.drop('Attrition', axis=1)\ny = data.Attrition\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,\n                                                    stratify=y)\n\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)\nX_test_std = scaler.transform(X_test)\nX_std = scaler.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_imp(df, model):\n    fi = pd.DataFrame()\n    fi[\"feature\"] = df.columns\n    fi[\"importance\"] = model.feature_importances_\n    return fi.sort_values(by=\"importance\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What defines success?\nWe have an imbalanced data, so if we predict that all our employees will stay we'll have an accuracy of `83.90%`. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.value_counts()[0] / y_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stay = (y_train.value_counts()[0] / y_train.shape)[0]\nleave = (y_train.value_counts()[1] / y_train.shape)[0]\n\nprint(\"===============TRAIN=================\")\nprint(f\"Staying Rate: {stay * 100:.2f}%\")\nprint(f\"Leaving Rate: {leave * 100 :.2f}%\")\n\nstay = (y_test.value_counts()[0] / y_test.shape)[0]\nleave = (y_test.value_counts()[1] / y_test.shape)[0]\n\nprint(\"===============TEST=================\")\nprint(f\"Staying Rate: {stay * 100:.2f}%\")\nprint(f\"Leaving Rate: {leave * 100 :.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(\"Classification Report:\", end='')\n        print(f\"\\tPrecision Score: {precision_score(y_train, pred) * 100:.2f}%\")\n        print(f\"\\t\\t\\tRecall Score: {recall_score(y_train, pred) * 100:.2f}%\")\n        print(f\"\\t\\t\\tF1 score: {f1_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(\"Classification Report:\", end='')\n        print(f\"\\tPrecision Score: {precision_score(y_test, pred) * 100:.2f}%\")\n        print(f\"\\t\\t\\tRecall Score: {recall_score(y_test, pred) * 100:.2f}%\")\n        print(f\"\\t\\t\\tF1 score: {f1_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. 1. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_classifier = LogisticRegression(solver='liblinear', penalty='l1')\nlr_classifier.fit(X_train_std, y_train)\n\nprint_score(lr_classifier, X_train_std, y_train, X_test_std, y_test, train=True)\nprint_score(lr_classifier, X_train_std, y_train, X_test_std, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. 2. Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrand_forest = RandomForestClassifier(n_estimators=1200, \n#                                      bootstrap=False,\n#                                      class_weight={0:stay, 1:leave}\n                                    )\nrand_forest.fit(X_train, y_train)\n\nprint_score(rand_forest, X_train, y_train, X_test, y_test, train=True)\nprint_score(rand_forest, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = feature_imp(X, rand_forest)[:40]\ndf.set_index('feature', inplace=True)\ndf.plot(kind='barh', figsize=(10, 10))\nplt.title('Feature Importance according to Random Forest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. 3. Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = SVC(kernel='linear')\nsvc.fit(X_train_std, y_train)\n\nprint_score(svc, X_train_std, y_train, X_test_std, y_test, train=True)\nprint_score(svc, X_train_std, y_train, X_test_std, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. 4. XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb_clf = XGBClassifier()\nxgb_clf.fit(X_train, y_train)\n\nprint_score(xgb_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(xgb_clf, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = feature_imp(X, xgb_clf)[:35]\ndf.set_index('feature', inplace=True)\ndf.plot(kind='barh', figsize=(10, 8))\nplt.title('Feature Importance according to XGBoost')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. 5. Artificial Neural Networks (ANNs)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_std = np.array(X_train_std)\nX_test_std = np.array(X_test_std)\ny_train = np.array(y_train)\ny_test = np.array(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_score_ann(label, prediction, train=True):\n    if train:\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(\"Classification Report:\", end='')\n        print(f\"\\tPrecision Score: {precision_score(label, prediction) * 100:.2f}%\")\n        print(f\"\\t\\t\\tRecall Score: {recall_score(label, prediction) * 100:.2f}%\")\n        print(f\"\\t\\t\\tF1 score: {f1_score(label, prediction) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, prediction)}\\n\")\n        \n    elif train==False:\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(\"Classification Report:\", end='')\n        print(f\"\\tPrecision Score: {precision_score(label, prediction) * 100:.2f}%\")\n        print(f\"\\t\\t\\tRecall Score: {recall_score(label, prediction) * 100:.2f}%\")\n        print(f\"\\t\\t\\tF1 score: {f1_score(label, prediction) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(label, prediction)}\\n\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\nmodel = Sequential()\n\nmodel.add(Dense(X_train.shape[1], activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', \n              loss='binary_crossentropy', \n              metrics=['accuracy'])\n\nr = model.fit(X_train_std, y_train, \n              validation_data=(X_test_std, y_test), \n              epochs=50, \n              class_weight={0:1, 1:2}\n             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_score = model.evaluate(X_train_std, y_train)\ntesting_score = model.evaluate(X_test_std, y_test)\n\nprint(f\"TRAINING SCORE: {training_score}\")\nprint(f\"TESTING SCORE: {testing_score}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(r.history['loss'], label='Loss')\nplt.plot(r.history['val_loss'], label='val_Loss')\nplt.legend()\n\nplt.subplot(2, 2, 2)\nplt.plot(r.history['acc'], label='acc')\nplt.plot(r.history['val_acc'], label='acc')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = model.predict(X_train_std)\ny_test_pred = model.predict(X_test_std)\n\nprint_score_ann(y_train, y_train_pred.round(), train=True)\nprint_score_ann(y_test, y_test_pred.round(), train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Balance the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('Attrition', axis=1)\ny = data.Attrition\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Concatinating X_train and y_train\ndf = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n\nminority_class = df[df.Attrition == 1]\nmajority_class = df[df.Attrition == 0]\n\nmajority_downsample = resample(majority_class, replace=False, \n                               n_samples=minority_class.shape[0], \n                               random_state=42)\n\ndf_2 = pd.concat([majority_downsample, minority_class])\ndf_2.Attrition.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_2.drop('Attrition', axis=1)\ny_train = df_2.Attrition\n\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)\nX_test_std = scaler.transform(X_test)\nX_std = scaler.transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. 1. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_classifier = LogisticRegression(solver='liblinear', penalty='l1')\nlr_classifier.fit(X_train_std, y_train)\n\nprint_score(lr_classifier, X_train_std, y_train, X_test_std, y_test, train=True)\nprint_score(lr_classifier, X_train_std, y_train, X_test_std, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. 2. Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_forest = RandomForestClassifier(n_estimators=1500, \n                                     bootstrap=True, \n                                     oob_score=True\n                                    )\nrand_forest.fit(X_train, y_train)\n\nprint_score(rand_forest, X_train, y_train, X_test, y_test, train=True)\nprint_score(rand_forest, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. 3. Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='linear')\nsvc.fit(X_train_std, y_train)\n\nprint_score(svc, X_train_std, y_train, X_test_std, y_test, train=True)\nprint_score(svc, X_train_std, y_train, X_test_std, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. 4. XGBoost classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_clf = XGBClassifier()\nxgb_clf.fit(X_train, y_train)\n\nprint_score(xgb_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(xgb_clf, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}